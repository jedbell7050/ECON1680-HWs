{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d3af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fuzzywuzzy import fuzz \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41545670",
   "metadata": {},
   "source": [
    "1.\tDownload data on covid tweets from Kaggle: https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/download  \n",
    "Load both the training and testing files into python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e521c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the two csvs and create training and testing dataframes\n",
    "train_df = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')\n",
    "test_df = pd.read_csv('Corona_NLP_test.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc48d3",
   "metadata": {},
   "source": [
    "2.\tPlot histogram of sentiment categories in the training dataframe. Hint: Use df[‘Sentiment’] .value_counts().reindex([\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]).plot(kind='bar') to control ordering of categories. Be sure to add title and labels for your axes. Insert your graph here.  \n",
    "\n",
    "\n",
    "3.\tPlot histogram of sentiment categories in the testing dataframe. Insert your graph here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b896b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the matplotlib object\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "# Plot the training histogram with the appropriate labels and set the title\n",
    "train_df['Sentiment'].value_counts().reindex([\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \n",
    "                                               \"Extremely Positive\"]).plot(kind='bar', ax=ax[0]) \n",
    "ax[0].set_title('Training Histogram')\n",
    "\n",
    "# Plot the testing histogram with the appropriate labels and set the title\n",
    "test_df['Sentiment'].value_counts().reindex([\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \n",
    "                                               \"Extremely Positive\"]).plot(kind='bar', ax=ax[1]) \n",
    "ax[1].set_title('Testing Histogram')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088947e",
   "metadata": {},
   "source": [
    "4.\tCompare the distributions. Do you think this is a good training/testing split? Why or why not? Write your answer here in 2-3 sentences.  \n",
    "\n",
    "\n",
    "This looks like a good training/testing split because the histograms look very similar, with spikes at both negative and positive, and dips at the other categories. The training dataset has a higher frequency of positive compared to negative than the testing dataset does, but it doesn't look like a significant difference and the two datasets don't need to be exactly identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f404b",
   "metadata": {},
   "source": [
    "5.\tFor this homework, you will be working with the testing dataset only because it is a smaller sample. Furthermore, drop all Neutral labelled tweets. Call this dataframe: df_tweet. How many observations (tweets) does this new dataframe have? Write your answer here.  \n",
    "\n",
    "df_tweet has 3179 observations remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02107496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName         3179\n",
       "ScreenName       3179\n",
       "Location         2467\n",
       "TweetAt          3179\n",
       "OriginalTweet    3179\n",
       "Sentiment        3179\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out the tweets with Neutral sentiment and print the counts\n",
    "df_tweet = test_df[test_df['Sentiment']!='Neutral'].reset_index(drop=True)\n",
    "df_tweet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f960ed2",
   "metadata": {},
   "source": [
    "6.\tTo clean the tweets a little, run the following code. NOTE: this code will take between 5-10 minutes to run on a laptop. Explain what each line is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb7a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates an empty list to store the new clean tweets in\n",
    "cleantweet=[]\n",
    "\n",
    "# this loops through all the tweets in the dataframe df_tweet\n",
    "for i in range(len(df_tweet)):\n",
    "    # This gets the tweet from the dataframe\n",
    "    tweet=df_tweet['OriginalTweet'][i]\n",
    "    \n",
    "    # These replace the given characters ('\\r', '\\n', '. ', and ', ' with a space)\n",
    "    tweet=tweet.replace('\\r',' ')\n",
    "    tweet=tweet.replace('\\n',' ')\n",
    "    tweet=tweet.replace('.  ',' ')\n",
    "    tweet=tweet.replace(', ',' ')\n",
    "    \n",
    "    # This splits each row on the spaces to create a list of words\n",
    "    tokens = tweet.split(' ')\n",
    "    \n",
    "    # This creates empty lists to store the hashtags and words\n",
    "    tweet_hashtags=[]\n",
    "    tweet_token=[]\n",
    "    \n",
    "    # This loops through all the words in the word list created from tweet.split\n",
    "    for t in tokens:\n",
    "        \n",
    "        # This checks if the word contains the hyperlink since they all have 'https:', and if it doesn't then it's\n",
    "        # added to the new list of words\n",
    "        if \"https:\" not in t:\n",
    "            tweet_token.append(t)\n",
    "            \n",
    "    # This adds the word to the clean tweet list and joins all the words from the list to reconstruct the \n",
    "    # tweet from the list of words, and it makes them all lowercase\n",
    "    cleantweet.append(\" \".join(tweet_token).lower())\n",
    "    \n",
    "# This creates a new column from the list of cleaned tweets\n",
    "df_tweet['clean tweet']= cleantweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd9ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Below Takes a few min to run. 5-10 min. ####\n",
    "\n",
    "# This sets the similarity threshold for dropping the tweet\n",
    "similar_level=85 \n",
    "\n",
    "# This creates an empty list to store indices of tweets that are to be deleted\n",
    "duplicate_index = []\n",
    "\n",
    "# This loops throuhg all the tweets in df_tweet except the last one\n",
    "for original in range(len(df_tweet)-1): \n",
    "    if original not in duplicate_index:\n",
    "        \n",
    "        # If the tweet is not in the duplicate index, then loop through all the tweets starting with the current\n",
    "        # one through the end of the dataframe\n",
    "        for compared in range(original+1, len(df_tweet)): \n",
    "            \n",
    "            # If the compared tweet isn't in the duplicate index, then check how similar the compared and original\n",
    "            # tweets are. If the ratio is greater than 85, add the compared tweet to the duplicate index\n",
    "            # to get dropped\n",
    "            if compared not in duplicate_index:                \n",
    "                if fuzz.ratio(df_tweet['clean tweet'][original], \n",
    "                              df_tweet['clean tweet'][compared]) >= similar_level: \n",
    "                    duplicate_index.append(compared)\n",
    "                    \n",
    "# The point of the above code is to drop tweets that are too similar from the dataframe\n",
    "            \n",
    "# This drops the tweets that are too similar and resets in the index of the dataframe\n",
    "df_tweet.drop(duplicate_index, inplace=True)\n",
    "df_tweet.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f33bcd",
   "metadata": {},
   "source": [
    "7.\tMake a list of hashtags for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2045005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the hashtages\n",
    "hashtags = []\n",
    "\n",
    "# Loop through the clean tweets and split up each word\n",
    "for index, item in df_tweet.iterrows():\n",
    "    words = item['clean tweet'].split(\" \")\n",
    "    \n",
    "    # Initialize and empty list for the tags\n",
    "    tags = []\n",
    "    # Loop throuhg every word in each tweet and if it contains a \"#\", add it to tags\n",
    "    for w in words:\n",
    "        if \"#\" in w:\n",
    "            tags.append(w)\n",
    "    \n",
    "    # Add each list of tags to hashtags\n",
    "    hashtags.append(tags)\n",
    "            \n",
    "# Set the new columns equal to hashtags\n",
    "df_tweet['hashtags'] = hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b83ac",
   "metadata": {},
   "source": [
    "8.\tWhat are the top 20 most used hashtags? Insert the list of hashtags and how many time each hashtag occur here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07d4221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#covid_19', 1215),\n",
       " ('#coronavirus', 1021),\n",
       " ('#covid19', 226),\n",
       " ('#coronaviruspandemic', 176),\n",
       " ('#covid2019', 141),\n",
       " ('#coronaoutbreak', 127),\n",
       " ('#coronapocalypse', 101),\n",
       " ('#covid?19', 88),\n",
       " ('#panicbuying', 83),\n",
       " ('#coronavirusupdate', 49),\n",
       " ('#coronavirusoutbreak', 43),\n",
       " ('#corona', 41),\n",
       " ('#toiletpaper', 34),\n",
       " ('#retail', 29),\n",
       " ('#pandemic', 28),\n",
       " ('#coronavirusupdates', 28),\n",
       " ('#coronavirus.', 27),\n",
       " ('#covid', 24),\n",
       " ('#food', 23),\n",
       " ('#lockdown', 23)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the counter to count each occurance of each hashtag and print the top 20 most common\n",
    "tag_counts = Counter(chain.from_iterable(hashtags))\n",
    "tag_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a44c3",
   "metadata": {},
   "source": [
    "9.\tUsing the ‘clean tweet’ variable from problem 6, create term frequency vector representation for all tweets. That is, make a document-term matrix where each document is a tweet and the terms are all the vocabulary from the tweet dataset. Hint: use TfidfVectorizer().fit_transform(). Call your document term matrix X. It should come out as a scipy.sparse.csr.csr_matrix object.   \n",
    "\n",
    "a.\tWhat are the dimensions of your matrix? Write that out here.  \n",
    "The dimensions are 3138 rows by 11074 columns, or 3138 documents with 11074 unique words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f9b11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3138x11074 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 93976 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and Tfidf vectorizer and fit it to the clean tweets\n",
    "tv = TfidfVectorizer()\n",
    "X = tv.fit_transform(df_tweet['clean tweet'])\n",
    "\n",
    "# Display the result to find the dimensions\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4accff",
   "metadata": {},
   "source": [
    "10.\tUse .get_feature_names() to make a list of the vocabulary and call it “features”.   \n",
    "a.\tIs this list alphabetized automatically? Write your answer here:  \n",
    "Yes, this list is automatically sorted into alphabetical order  \n",
    "\n",
    "b.\tWhat is the word associated with index 1680? Write your answer here:  \n",
    "'caramel' is associated with the index 1680\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ebf78bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aka', 'aka_layla', 'akin', 'akin_adesina', 'al', 'alabama', 'alarm', 'alas', 'alaska', 'albeit', 'albert', 'alberta', 'albertarecession', 'alcohol', 'alcohol_based_product', 'alcohols', 'alcoholâ', 'aldi', 'aldiuk', 'aldiusa', 'alert', 'alerting', 'alertness', 'alex', 'alex_carrick']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'caramel'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the features using get_feature_names and print out a subset to check if they are in alphabetical order\n",
    "features = tv.get_feature_names()\n",
    "print(features[550:575])\n",
    "\n",
    "# Display the item at index 1680\n",
    "features[1680]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ee8f0",
   "metadata": {},
   "source": [
    "11.\tCalculate the pairwise similarity between all tweets using cosine similarity. Hint: use the cosine_similarity() command from sklearn and then change it to a dataframe called df_cossim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df728c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the similarity dataframe with the tweet indices as column labels\n",
    "df_cossim = pd.DataFrame(cosine_similarity(X), df_tweet.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0efff",
   "metadata": {},
   "source": [
    "12.\tRun the following code and explain each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bad9a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a dataframe of the values from the triangle under the diagonal from the cosine similarity dataframe \n",
    "df_cossim_tri = pd.DataFrame(np.tril(df_cossim.values, k=-1))\n",
    "\n",
    "# This flattens the dataframe so each comparison is it's own row, so we can easily rank the similarity scores\n",
    "df_rank = df_cossim_tri.unstack().reset_index()\n",
    "\n",
    "# This names the columns of the dataframe\n",
    "df_rank.columns = ['row', 'column', 'similarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11932f",
   "metadata": {},
   "source": [
    "13.\tWhich pairs of tweets have the top 10 highest similarity scores? Below include a table with the tweet pairs and their corresponding similarity score. Hint: sort the df_rank dataframe over similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9d14a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2646686</th>\n",
       "      <td>843</td>\n",
       "      <td>1352</td>\n",
       "      <td>0.852782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546270</th>\n",
       "      <td>811</td>\n",
       "      <td>1352</td>\n",
       "      <td>0.852692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560513</th>\n",
       "      <td>178</td>\n",
       "      <td>1949</td>\n",
       "      <td>0.823141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244517</th>\n",
       "      <td>1352</td>\n",
       "      <td>1941</td>\n",
       "      <td>0.814792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7452073</th>\n",
       "      <td>2374</td>\n",
       "      <td>2461</td>\n",
       "      <td>0.813846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182325</th>\n",
       "      <td>58</td>\n",
       "      <td>321</td>\n",
       "      <td>0.812643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546388</th>\n",
       "      <td>811</td>\n",
       "      <td>1470</td>\n",
       "      <td>0.811657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546859</th>\n",
       "      <td>811</td>\n",
       "      <td>1941</td>\n",
       "      <td>0.804707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545761</th>\n",
       "      <td>811</td>\n",
       "      <td>843</td>\n",
       "      <td>0.801811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039048</th>\n",
       "      <td>2561</td>\n",
       "      <td>2630</td>\n",
       "      <td>0.795819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          row  column  similarity\n",
       "2646686   843    1352    0.852782\n",
       "2546270   811    1352    0.852692\n",
       "560513    178    1949    0.823141\n",
       "4244517  1352    1941    0.814792\n",
       "7452073  2374    2461    0.813846\n",
       "182325     58     321    0.812643\n",
       "2546388   811    1470    0.811657\n",
       "2546859   811    1941    0.804707\n",
       "2545761   811     843    0.801811\n",
       "8039048  2561    2630    0.795819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the similarity values in descending order\n",
    "df_rank.sort_values('similarity', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9f0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call your rep amp senators today amp demand they pass the bill it contains free testing 14 days paid sick leave 3 months paid family leave expanded unemployment insurance amp food security medicaid funds lives are on the line 202 224 3121\n",
      "call your representatives today and demand they pass the #familiesfirst coronavirus bill. #coronavirus    - free testing   - 14 days paid sick leave   - 3 months paid family leave   - expanded unemployment insurance &amp; food security   - medicaid funds   lives are on the line. (202) 224-3121\n",
      "\n",
      "call now 202 224 3121 and demand congress pass the bill   free testing   14 days paid sick leave   3 months paid family leave   expanded unemployment insurance and food security   medicaid funds\n",
      "call your representatives today and demand they pass the #familiesfirst coronavirus bill. #coronavirus    - free testing   - 14 days paid sick leave   - 3 months paid family leave   - expanded unemployment insurance &amp; food security   - medicaid funds   lives are on the line. (202) 224-3121\n",
      "\n",
      "coronavirus is hurting store traffic and e-commerce won't help much #retail #coronavirus \n",
      "coronavirus is hurting store traffic and e-commerce won't help much analysts say #coronavirus #retail #ecommerce\n",
      "\n",
      "call your representatives today and demand they pass the #familiesfirst coronavirus bill. #coronavirus    - free testing   - 14 days paid sick leave   - 3 months paid family leave   - expanded unemployment insurance &amp; food security   - medicaid funds   lives are on the line. (202) 224-3121\n",
      "call your senators. demand they pass the #familiesfirst #coronavirus bill containing:      -free testing   -14 days paid sick leave   -3 months paid family leave   -expanded unemployment insurance -food security   -medicaid funds       ask for your senatorâs office. (202) 224-3121 #covid19\n",
      "\n",
      "went to the supermarket amp i d like to remind everyone that 19 is a respiratory disease not assitory disease please stop hoarding the toilet paper thanks\n",
      "people are annoying af       rt @daisyfuentes: went to the supermarket &amp; iâd like to remind everyone that #covid_19 is a respiratory disease not assitory disease. please stop hoarding the toilet paper. thanks.\n",
      "\n",
      "trump said people must \"be vigilant,\" then concluded \"be calm. it's really working out. and: lot of good things are going to happen. the consumer is ready. the consumer is so powerful in our country with what we've done with tax cuts and regulation cuts and all of those things.\"\n",
      "\"itâs really working out. and a lot of good things are going to happen. the consumer is ready and the consumer is so powerful in our country with what weâve done with tax cuts and regulation cuts and all of those things.\" #covid19 #covid_19 #stablegenius \n",
      "\n",
      "call now 202 224 3121 and demand congress pass the bill   free testing   14 days paid sick leave   3 months paid family leave   expanded unemployment insurance and food security   medicaid funds\n",
      "@cristela9 we need paid sick days &amp; leave. call now: (202) 224-3121 and demand congress pass the #familiesfirst #coronavirus bill     ?free testing    ?  paid sick leave   ???????3 months paid family leave   ? expanded unemployment insurance and foo\n",
      "\n",
      "call now 202 224 3121 and demand congress pass the bill   free testing   14 days paid sick leave   3 months paid family leave   expanded unemployment insurance and food security   medicaid funds\n",
      "call your senators. demand they pass the #familiesfirst #coronavirus bill containing:      -free testing   -14 days paid sick leave   -3 months paid family leave   -expanded unemployment insurance -food security   -medicaid funds       ask for your senatorâs office. (202) 224-3121 #covid19\n",
      "\n",
      "call now 202 224 3121 and demand congress pass the bill   free testing   14 days paid sick leave   3 months paid family leave   expanded unemployment insurance and food security   medicaid funds\n",
      "call your rep amp senators today amp demand they pass the bill it contains free testing 14 days paid sick leave 3 months paid family leave expanded unemployment insurance amp food security medicaid funds lives are on the line 202 224 3121\n",
      "\n",
      "as the push for food continues to grow during the covid 19 outbreak those who need it most may have a hard time getting it\n",
      "thanks for coming by our food shelf today rt @wcco: as the push for food continues to grow during the covid-19 outbreak those who need it most may have a hard time getting it. |\n"
     ]
    }
   ],
   "source": [
    "# Print out each of the 10 pairs to do a manual check of the similarities\n",
    "print(df_tweet['clean tweet'][843])\n",
    "print(df_tweet['clean tweet'][1352])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][811])\n",
    "print(df_tweet['clean tweet'][1352])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][178])\n",
    "print(df_tweet['clean tweet'][1949])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][1352])\n",
    "print(df_tweet['clean tweet'][1941])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][2374])\n",
    "print(df_tweet['clean tweet'][2461])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][58])\n",
    "print(df_tweet['clean tweet'][321])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][811])\n",
    "print(df_tweet['clean tweet'][1470])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][811])\n",
    "print(df_tweet['clean tweet'][1941])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][811])\n",
    "print(df_tweet['clean tweet'][843])\n",
    "print()\n",
    "print(df_tweet['clean tweet'][2561])\n",
    "print(df_tweet['clean tweet'][2630])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9162b8",
   "metadata": {},
   "source": [
    "14.\tSpend 3-5 sentences describing what seems to make sense in the similarity matching. What doesn’t make sense? If you wanted to improve matching, what steps would you take now? If you would need to do more cleaning of the data, what would you want to do?  \n",
    "\n",
    "\n",
    "It seems that tweets that repeat the same phrases are more likely to have higher similairty scores, which makes sense. For example, the only differences between the top two most similar tweets are some different abbreviations and puncuation. Since these tweets are nearly identical, it seems that the similarity score should actually be higher. This could be achieved by removing more kinds of punctuation than just periods and commas and by stemming certain words, especially those that are often abbreviated over tweets like 'reps' for 'representatives'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43812f36",
   "metadata": {},
   "source": [
    "15.\tMake a new variable that has binary sentiment (positive=1 and negative=0) where a sentiment of positive and extremely positive is now 1, and then negative and extremely negative is now 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e5d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and empty list to store the binary sentiment rating\n",
    "bin_sent = []\n",
    "\n",
    "# Loop through each item in the sentiment column and add 1 to bin_sent if the sentiment is Ppositive or\n",
    "# extremely positive, otherwise add 0\n",
    "for index, item in df_tweet.iterrows():\n",
    "    if 'Positive' in item['Sentiment']:\n",
    "        bin_sent.append(1)\n",
    "    else:\n",
    "        bin_sent.append(0)\n",
    "        \n",
    "# Create the new column bin_sent from the list of binary sentiment\n",
    "df_tweet['bin_sent'] = bin_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c33b8b",
   "metadata": {},
   "source": [
    "16.\tRun a logit lasso regression with binary sentiment as the output variable and your document term matrix as your inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f70fcb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.49363057324840764, 0.001), (0.49363057324840764, 0.01), (0.49363057324840764, 0.1), (0.7101910828025477, 1), (0.7691082802547771, 10), (0.767515923566879, 100), (0.7627388535031847, 1000)]\n"
     ]
    }
   ],
   "source": [
    "# Create the document term matrix and save it as x, and save the response variable, binary sentiment, as y\n",
    "x = pd.DataFrame(X.toarray(), df_tweet.index)\n",
    "y = df_tweet['bin_sent']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1680)\n",
    "\n",
    "# The C values to loop through\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "# Empty list to store accuracies\n",
    "acc = []\n",
    "# Loop through each C, fit the model, and store the accuracy and C value as a tuple\n",
    "for c in Cs:\n",
    "    logit = LogisticRegression(penalty='l1', solver='liblinear', random_state=1680, C=c)\n",
    "    model = logit.fit(X_train,y_train)\n",
    "    acc.append((model.score(X_test,y_test), c))\n",
    "    \n",
    "print(acc)\n",
    "# C = 10 gives the best accuracy of 0.7691"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635977a8",
   "metadata": {},
   "source": [
    "16 d.\tRun the following code and explain each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c99ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with the optimal C value (C=10) and save the coefficients\n",
    "logit = LogisticRegression(penalty='l1', solver='liblinear', random_state=1680, C=10)\n",
    "model = logit.fit(x,y)\n",
    "coefs = model.coef_\n",
    "\n",
    "# This creates a dataframe with every word and its coefficient\n",
    "df_coef = pd.DataFrame({'word': features, 'coef': coefs[-1].tolist()})\n",
    "\n",
    "# This adds a column to the coefficient dataframe that is the absolute value of the coefficient\n",
    "df_coef['abs coef']= df_coef['coef'].abs()\n",
    "\n",
    "# This adds a column to the tweet dataframe that is the predicted sentiment of the tweet\n",
    "df_tweet['logitlasso predicted sentiment'] = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5d0d5",
   "metadata": {},
   "source": [
    "17.\tWhat are the top 20 most important words for predicting class? These will be the largest coefficients in absolute value. Hint: sort df_coef. Print those top 20 words and their coefficients here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8fa901e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>resource</td>\n",
       "      <td>-24.903412</td>\n",
       "      <td>24.903412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>hand</td>\n",
       "      <td>23.218213</td>\n",
       "      <td>23.218213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7107</th>\n",
       "      <td>panic</td>\n",
       "      <td>-22.791281</td>\n",
       "      <td>22.791281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>crisis</td>\n",
       "      <td>-22.565036</td>\n",
       "      <td>22.565036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>great</td>\n",
       "      <td>21.988188</td>\n",
       "      <td>21.988188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9614</th>\n",
       "      <td>suspended</td>\n",
       "      <td>-21.494135</td>\n",
       "      <td>21.494135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>best</td>\n",
       "      <td>21.484082</td>\n",
       "      <td>21.484082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>free</td>\n",
       "      <td>20.436642</td>\n",
       "      <td>20.436642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>safety</td>\n",
       "      <td>20.272930</td>\n",
       "      <td>20.272930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>positive</td>\n",
       "      <td>19.637654</td>\n",
       "      <td>19.637654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>yeah</td>\n",
       "      <td>19.010867</td>\n",
       "      <td>19.010867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>like</td>\n",
       "      <td>18.970383</td>\n",
       "      <td>18.970383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9473</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-18.843976</td>\n",
       "      <td>18.843976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>friends</td>\n",
       "      <td>18.538128</td>\n",
       "      <td>18.538128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>happy</td>\n",
       "      <td>18.511778</td>\n",
       "      <td>18.511778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336</th>\n",
       "      <td>good</td>\n",
       "      <td>18.031770</td>\n",
       "      <td>18.031770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>wrong</td>\n",
       "      <td>-17.942282</td>\n",
       "      <td>17.942282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7165</th>\n",
       "      <td>partner</td>\n",
       "      <td>-17.677614</td>\n",
       "      <td>17.677614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>healthy</td>\n",
       "      <td>17.485899</td>\n",
       "      <td>17.485899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>sick</td>\n",
       "      <td>-17.462783</td>\n",
       "      <td>17.462783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word       coef   abs coef\n",
       "8222    resource -24.903412  24.903412\n",
       "4546        hand  23.218213  23.218213\n",
       "7107       panic -22.791281  22.791281\n",
       "2602      crisis -22.565036  22.565036\n",
       "4413       great  21.988188  21.988188\n",
       "9614   suspended -21.494135  21.494135\n",
       "1207        best  21.484082  21.484082\n",
       "4095        free  20.436642  20.436642\n",
       "8470      safety  20.272930  20.272930\n",
       "7511    positive  19.637654  19.637654\n",
       "10997       yeah  19.010867  19.010867\n",
       "5755        like  18.970383  18.970383\n",
       "9473      stupid -18.843976  18.843976\n",
       "4130     friends  18.538128  18.538128\n",
       "4575       happy  18.511778  18.511778\n",
       "4336        good  18.031770  18.031770\n",
       "10952      wrong -17.942282  17.942282\n",
       "7165     partner -17.677614  17.677614\n",
       "4638     healthy  17.485899  17.485899\n",
       "8912        sick -17.462783  17.462783"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 words\n",
    "df_coef.sort_values('abs coef', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed6144b",
   "metadata": {},
   "source": [
    "18.\tWhat are the top 10 “positive words”? The top 10 “negative words”? Print the top 10 words of each sentiment and their coefficients here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc8e8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>hand</td>\n",
       "      <td>23.218213</td>\n",
       "      <td>23.218213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>great</td>\n",
       "      <td>21.988188</td>\n",
       "      <td>21.988188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>best</td>\n",
       "      <td>21.484082</td>\n",
       "      <td>21.484082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>free</td>\n",
       "      <td>20.436642</td>\n",
       "      <td>20.436642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>safety</td>\n",
       "      <td>20.272930</td>\n",
       "      <td>20.272930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>positive</td>\n",
       "      <td>19.637654</td>\n",
       "      <td>19.637654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>yeah</td>\n",
       "      <td>19.010867</td>\n",
       "      <td>19.010867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>like</td>\n",
       "      <td>18.970383</td>\n",
       "      <td>18.970383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>friends</td>\n",
       "      <td>18.538128</td>\n",
       "      <td>18.538128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>happy</td>\n",
       "      <td>18.511778</td>\n",
       "      <td>18.511778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word       coef   abs coef\n",
       "4546       hand  23.218213  23.218213\n",
       "4413      great  21.988188  21.988188\n",
       "1207       best  21.484082  21.484082\n",
       "4095       free  20.436642  20.436642\n",
       "8470     safety  20.272930  20.272930\n",
       "7511   positive  19.637654  19.637654\n",
       "10997      yeah  19.010867  19.010867\n",
       "5755       like  18.970383  18.970383\n",
       "4130    friends  18.538128  18.538128\n",
       "4575      happy  18.511778  18.511778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>resource</td>\n",
       "      <td>-24.903412</td>\n",
       "      <td>24.903412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7107</th>\n",
       "      <td>panic</td>\n",
       "      <td>-22.791281</td>\n",
       "      <td>22.791281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>crisis</td>\n",
       "      <td>-22.565036</td>\n",
       "      <td>22.565036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9614</th>\n",
       "      <td>suspended</td>\n",
       "      <td>-21.494135</td>\n",
       "      <td>21.494135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9473</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-18.843976</td>\n",
       "      <td>18.843976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>wrong</td>\n",
       "      <td>-17.942282</td>\n",
       "      <td>17.942282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7165</th>\n",
       "      <td>partner</td>\n",
       "      <td>-17.677614</td>\n",
       "      <td>17.677614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>sick</td>\n",
       "      <td>-17.462783</td>\n",
       "      <td>17.462783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>ffs</td>\n",
       "      <td>-17.168007</td>\n",
       "      <td>17.168007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>hell</td>\n",
       "      <td>-16.892468</td>\n",
       "      <td>16.892468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word       coef   abs coef\n",
       "8222    resource -24.903412  24.903412\n",
       "7107       panic -22.791281  22.791281\n",
       "2602      crisis -22.565036  22.565036\n",
       "9614   suspended -21.494135  21.494135\n",
       "9473      stupid -18.843976  18.843976\n",
       "10952      wrong -17.942282  17.942282\n",
       "7165     partner -17.677614  17.677614\n",
       "8912        sick -17.462783  17.462783\n",
       "3843         ffs -17.168007  17.168007\n",
       "4667        hell -16.892468  16.892468"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 Positive\n",
    "display(df_coef.sort_values('coef', ascending=False).head(10))\n",
    "\n",
    "# Top 10 Negative\n",
    "display(df_coef.sort_values('coef').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c28e7",
   "metadata": {},
   "source": [
    "19.\tHere you will use the HuggingFace package with pre-trained algorithms. This pipeline takes string text as input, not tokens. So you will be using your dataframe from question 5. First work with the default model for sentiment analysis from HuggingFace which is the DistilBERT base uncased model finetuned with SST-2.  \n",
    "\n",
    "\n",
    "a.\tRun the following code  \n",
    "\n",
    "\n",
    "b.\tPaste the output from the code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e492beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.999046266078949}, {'label': 'NEGATIVE', 'score': 0.9964587092399597}, {'label': 'POSITIVE', 'score': 0.9808228611946106}, {'label': 'NEGATIVE', 'score': 0.9943122863769531}, {'label': 'NEGATIVE', 'score': 0.9957156777381897}, {'label': 'NEGATIVE', 'score': 0.9990513920783997}, {'label': 'POSITIVE', 'score': 0.9799405932426453}, {'label': 'NEGATIVE', 'score': 0.9910359382629395}, {'label': 'NEGATIVE', 'score': 0.9969425797462463}, {'label': 'NEGATIVE', 'score': 0.9989578723907471}]\n",
      "TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-out online grocers (FoodKick, MaxDelivery) as #coronavirus-fearing shoppers stock up https://t.co/Gr76pcrLWh https://t.co/ivMKMsqdT1\n",
      "/n/break 0\n",
      "When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\n",
      "/n/break 1\n",
      "Find out how you can protect yourself and loved ones from #coronavirus. ?\n",
      "/n/break 2\n",
      "#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/IASiReGPC4\r",
      "\r\n",
      "\r",
      "\r\n",
      "#QAnon #QAnon2018 #QAnon2020 \r",
      "\r\n",
      "#Election2020 #CDC https://t.co/29isZOewxu\n",
      "/n/break 3\n",
      "Voting in the age of #coronavirus = hand sanitizer ? #SuperTuesday https://t.co/z0BeL4O6Dk\n",
      "/n/break 4\n",
      "HI TWITTER! I am a pharmacist. I sell hand sanitizer for a living! Or I do when any exists. Like masks, it is sold the fuck out everywhere. SHOULD YOU BE WORRIED? No. Use soap. SHOULD YOU VISIT TWENTY PHARMACIES LOOKING FOR THE LAST BOTTLE? No. Pharmacies are full of sick people.\n",
      "/n/break 5\n",
      "Anyone been in a supermarket over the last few days? Went to do my NORMAL shop last night &amp; ??is the sight that greeted me. Barmy! (Btw, whatÂs so special about tinned tomatoes? ????????????). #Covid_19 #Dublin https://t.co/rGsM8xUxr6\n",
      "/n/break 6\n",
      "Best quality couches at unbelievably low prices available to order.\r",
      "\r\n",
      "\r",
      "\r\n",
      "We are in Boksburg GP \r",
      "\r\n",
      "\r",
      "\r\n",
      "For more info WhatsApp:\r",
      "\r\n",
      "084 764 8086\r",
      "\r\n",
      "\r",
      "\r\n",
      "#SuperTuesdsy #PowerTalk \r",
      "\r\n",
      "#Covid_19 #SayEntrepreneur \r",
      "\r\n",
      "#DJSBU https://t.co/HhDJhyQ2Dc\n",
      "/n/break 7\n",
      "Beware of counterfeits trying to sell fake masks at cheap prices. Let's defeat coronavirus threat, #Covid_19 collectively. #BeSafe #BeACascader #CoronavirusReachesDelhi \r",
      "\r\n",
      "#coronavirusindia \r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/2Ikkmimj4f https://t.co/RB9rtt7Nkc\n",
      "/n/break 8\n",
      "Panic food buying in Germany due to #coronavirus has begun.  But the #organic is left behind! #Hamsterkauf\r",
      "\r\n",
      "\r",
      "\r\n",
      "Panic buying is called \"Hamster purchases\"(HamsterkÃ¤ufe) in German, taken from the way Hamsters stuff their cheeks with food.  \r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/aYQtLLGW1m\n",
      "/n/break 9\n"
     ]
    }
   ],
   "source": [
    "# Setup the default pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Print the results of running the pipeline on the original tweets\n",
    "print(sentiment_pipeline(df_tweet['OriginalTweet'][0:10].tolist()))\n",
    "\n",
    "# Print the original tweets with defined breaks for readability\n",
    "for i in range(len(df_tweet['OriginalTweet'][0:10].tolist())):\n",
    "    print(df_tweet['OriginalTweet'][0:10].tolist()[i])\n",
    "    print('/n/break ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168c96f",
   "metadata": {},
   "source": [
    "c.\tHow does this compare to the original sentiment label? Print out the original sentiment label for the first 10 tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd83982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Extremely Negative\n",
       "1              Positive\n",
       "2    Extremely Positive\n",
       "3              Negative\n",
       "4              Positive\n",
       "5    Extremely Negative\n",
       "6    Extremely Positive\n",
       "7              Positive\n",
       "8    Extremely Negative\n",
       "9    Extremely Negative\n",
       "Name: Sentiment, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the original sentiment labels for the first 10 tweets\n",
    "df_tweet['Sentiment'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c7a54",
   "metadata": {},
   "source": [
    "d.\tDo you think the hand coded labels (the original sentiment labels) or the ones predicted by the pre-trained algorithm make more sense? Explain your answer using the text from the tweets as evidence. Why do you think there are differences?  \n",
    "\n",
    "The hand coded labels mark tweets at indices 0,3,5,8,9 as negative, and tweets 1,2,4,6,7 as positive. The pre-trained algorithm marks tweets 0,1,3,4,5,7,8,9 as negative and tweets 2,6 as positive, so the two labelling methods disagree on tweets 1,4 and 7. For tweet 1, the pre-trained label makes more sense, but for tweet 7, the hand coded one does. Tweet 1 is talking about rising prices of hand sanitizer, which is certainly a negative thing especially during a pandemic. Tweet 7 is an advertisement emphasizing low prices, which is a good thing, but the tweet has a covid hashtag, which is probably severely negatively skewing the sentiment for the pre-trained model. The sentiment of tweet 4 seems pretty ambiguous as it's mostly just expressing an observation, and there are both positive things, like voting, and negative things, like coronavirus, mentioned in the tweet. The coronavirus mention is probably why the pre-trained model rates it as negative, while the person's tone in the attached video is more upbeat which is probably why it earns the hand coded label of positive. The latter is probably the more correct reading of the tweet.\n",
    "\n",
    "The ratings of all the other tweets seem to make sense for both models except 6. The negative tweets are almost all about shopping crises brought on by the beginning of covid, while the positive tweet is about protecting yourself and your family from covid. Both models seems to miss the sarcasm implied (and the included picture) of tweet 6, which is again about a shopping crisis.\n",
    "\n",
    "Overall, the hand coded labels seem to make slightly more sense, but neither model is perfect and the pre-trained model still performs very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d693e2",
   "metadata": {},
   "source": [
    "20.\tLook at the other pretrained models in HuggingFace at https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment. \n",
    "\n",
    "a.\tWhich pretrained models would be appropriate for analyzing the Covid Tweets dataset? Name the model you would use, broadly how it was trained, and your reasoning for why you would pick it.  \n",
    "\n",
    "Any model recently trained (i.e. post Covid-19) on large samples of English tweets should suffice for analyzing the Covid Tweets dataset. I chose the cardiffnlp/twitter-roberta-base-sentiment-latest, which was trained on 124 million tweets from January 2018 to December 2021, because this includes the entire context of the Covid crises so it will probably have the most accurate understanding of how mentions of covid relate to sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1497f98",
   "metadata": {},
   "source": [
    "b.\tTake the code from 19a. and run the sentiment analysis for the same 10 tweets. Print the predicted labels here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "391dcf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative', 'score': 0.5433744788169861}, {'label': 'Negative', 'score': 0.7914344072341919}, {'label': 'Neutral', 'score': 0.8296441435813904}, {'label': 'Negative', 'score': 0.5996304154396057}, {'label': 'Neutral', 'score': 0.7118917107582092}, {'label': 'Negative', 'score': 0.848591685295105}, {'label': 'Positive', 'score': 0.6536583304405212}, {'label': 'Neutral', 'score': 0.5237236618995667}, {'label': 'Negative', 'score': 0.5035489797592163}, {'label': 'Negative', 'score': 0.5594478249549866}]\n",
      "TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-out online grocers (FoodKick, MaxDelivery) as #coronavirus-fearing shoppers stock up https://t.co/Gr76pcrLWh https://t.co/ivMKMsqdT1\n",
      "/n/break 0\n",
      "When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\n",
      "/n/break 1\n",
      "Find out how you can protect yourself and loved ones from #coronavirus. ?\n",
      "/n/break 2\n",
      "#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/IASiReGPC4\r",
      "\r\n",
      "\r",
      "\r\n",
      "#QAnon #QAnon2018 #QAnon2020 \r",
      "\r\n",
      "#Election2020 #CDC https://t.co/29isZOewxu\n",
      "/n/break 3\n",
      "Voting in the age of #coronavirus = hand sanitizer ? #SuperTuesday https://t.co/z0BeL4O6Dk\n",
      "/n/break 4\n",
      "HI TWITTER! I am a pharmacist. I sell hand sanitizer for a living! Or I do when any exists. Like masks, it is sold the fuck out everywhere. SHOULD YOU BE WORRIED? No. Use soap. SHOULD YOU VISIT TWENTY PHARMACIES LOOKING FOR THE LAST BOTTLE? No. Pharmacies are full of sick people.\n",
      "/n/break 5\n",
      "Anyone been in a supermarket over the last few days? Went to do my NORMAL shop last night &amp; ??is the sight that greeted me. Barmy! (Btw, whatÂs so special about tinned tomatoes? ????????????). #Covid_19 #Dublin https://t.co/rGsM8xUxr6\n",
      "/n/break 6\n",
      "Best quality couches at unbelievably low prices available to order.\r",
      "\r\n",
      "\r",
      "\r\n",
      "We are in Boksburg GP \r",
      "\r\n",
      "\r",
      "\r\n",
      "For more info WhatsApp:\r",
      "\r\n",
      "084 764 8086\r",
      "\r\n",
      "\r",
      "\r\n",
      "#SuperTuesdsy #PowerTalk \r",
      "\r\n",
      "#Covid_19 #SayEntrepreneur \r",
      "\r\n",
      "#DJSBU https://t.co/HhDJhyQ2Dc\n",
      "/n/break 7\n",
      "Beware of counterfeits trying to sell fake masks at cheap prices. Let's defeat coronavirus threat, #Covid_19 collectively. #BeSafe #BeACascader #CoronavirusReachesDelhi \r",
      "\r\n",
      "#coronavirusindia \r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/2Ikkmimj4f https://t.co/RB9rtt7Nkc\n",
      "/n/break 8\n",
      "Panic food buying in Germany due to #coronavirus has begun.  But the #organic is left behind! #Hamsterkauf\r",
      "\r\n",
      "\r",
      "\r\n",
      "Panic buying is called \"Hamster purchases\"(HamsterkÃ¤ufe) in German, taken from the way Hamsters stuff their cheeks with food.  \r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/aYQtLLGW1m\n",
      "/n/break 9\n"
     ]
    }
   ],
   "source": [
    "# Set up the roberta-base-sentiment model in the pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                             tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Print the results of running the pipeline on the original tweets\n",
    "print(sentiment_pipeline(df_tweet['OriginalTweet'][0:10].tolist()))\n",
    "\n",
    "# Print the original tweets with defined breaks for readability\n",
    "for i in range(len(df_tweet['OriginalTweet'][0:10].tolist())):\n",
    "    print(df_tweet['OriginalTweet'][0:10].tolist()[i])\n",
    "    print('/n/break ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19456c",
   "metadata": {},
   "source": [
    "c.\tHow do the predicted labels here compare to those from question 19? Explain why you think there are differences/similarities.   \n",
    "\n",
    "This model assigns a negative sentiment for tweets 0,1,3,5,8,9, neutral for tweets 2,4,7 and positive for tweet 6.\n",
    "I think the neutral label certainly makes more sense for 4 and is probably reasonable for tweets 2 and 7 as well. The negative tweets all use obviously negative words like \"panic\", or \"beware\", or \"fear\", which is why they are still marked negative with this model. In the time since covid started, the word covid itself has become less of an inherently negative word and instead more neutral, which explains why this model marks tweets 4 and 7 as neutral instead of negative like the previous one. Again, the model seems to the miss the sarcasm in tweet 6 and mark it as positive. This might be because Dublin has a positive association in the training set, which could be skewing both models when there aren't any obviously negative words in the tweet (the default model documentation noted this could happen with certain country names).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d308f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
